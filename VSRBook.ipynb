{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data (run this at start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from PIL import Image\n",
    "from transformers.image_utils import load_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"train.jsonl\", \"dev\": \"dev.jsonl\", \"test\": \"test.jsonl\"}\n",
    "dataset = load_dataset(\"cambridgeltl/vsr_random\", data_files=data_files)\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "val_ds = dataset[\"dev\"]\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"HuggingFaceTB/SmolVLM-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1D1O6nKHOlO"
   },
   "source": [
    "## Testing out performance pre fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1D1O6nKHOlO"
   },
   "source": [
    "First we load a sample image from the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "QMIEPUqiJ_ru",
    "outputId": "aba7a061-f38f-45a8-8d8a-53506d143d0e"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got visual-spatial-reasoning/images\\000000262118.jpg. Failed with cannot identify image file <_io.BytesIO object at 0x0000018628E68220>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\image_utils.py:373\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image, timeout)\u001b[0m\n\u001b[0;32m    372\u001b[0m     b64 \u001b[39m=\u001b[39m base64\u001b[39m.\u001b[39mdecodebytes(image\u001b[39m.\u001b[39mencode())\n\u001b[1;32m--> 373\u001b[0m     image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(BytesIO(b64))\n\u001b[0;32m    374\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\bhara\\miniconda3\\envs\\snowflakes\\lib\\site-packages\\PIL\\Image.py:3283\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3282\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[1;32m-> 3283\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x0000018628E68220>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mvisual-spatial-reasoning/images\u001b[39m\u001b[39m\"\u001b[39m, train_ds[\u001b[39m2\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m img \u001b[39m=\u001b[39m load_image(img_path)\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mimshow(img)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\image_utils.py:375\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image, timeout)\u001b[0m\n\u001b[0;32m    373\u001b[0m             image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mopen(BytesIO(b64))\n\u001b[0;32m    374\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 375\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    376\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m}\u001b[39;00m\u001b[39m. Failed with \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m             )\n\u001b[0;32m    378\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m    379\u001b[0m     image \u001b[39m=\u001b[39m image\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got visual-spatial-reasoning/images\\000000262118.jpg. Failed with cannot identify image file <_io.BytesIO object at 0x0000018628E68220>"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(\"visual-spatial-reasoning/images\", train_ds[2][\"image\"])\n",
    "img = load_image(img_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what the caption and label for this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bed is right of the bench.\n",
      "1\n",
      "right of\n",
      "bed bench.\n"
     ]
    }
   ],
   "source": [
    "caption = train_ds[2][\"caption\"]\n",
    "label = train_ds[2][\"label\"]\n",
    "relation = train_ds[2][\"relation\"]\n",
    "\n",
    "print(caption)\n",
    "print(label)\n",
    "print(relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the SmolVLM-Instruct model, which is the Base model finetuned for handling structured prompts/questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71GOcMA5HNVj",
    "outputId": "aebb8e40-2537-4cb1-d710-a757320503fb"
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id,\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\").to(DEVICE)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "fZIlIA_SJ6U_",
    "outputId": "a7c01355-e0df-4a4d-c160-3b20afbef379"
   },
   "outputs": [],
   "source": [
    "def run_inference(img_path, caption):\n",
    "  # Load images\n",
    "  img = load_image(img_path)\n",
    " \n",
    "  # Create input messages\n",
    "  messages = [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"image\"},\n",
    "              {\"type\": \"text\", \"text\": f\"{caption.rstrip('.')}, true or false?\"}\n",
    "          ]\n",
    "      },\n",
    "  ]\n",
    "\n",
    "  # Prepare inputs\n",
    "  prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "  inputs = processor(text=prompt, images=[img], return_tensors=\"pt\")\n",
    "  inputs = inputs.to(DEVICE)\n",
    "\n",
    "  # Generate outputs\n",
    "  with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "  generated_texts = processor.batch_decode(\n",
    "      generated_ids,\n",
    "      skip_special_tokens=True,\n",
    "  )\n",
    "\n",
    "  return generated_texts[0]\n",
    "\n",
    "output = run_inference(img_path, caption)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model predicts correctly, since the bed is on the right side of the bench (from the perspective of the bench). Checkout line 3 of visual-spatial-reasoning/data/split/random/train.jsonl for the ground truth label (its 1 for true)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bDuI6tLqd4zs"
   },
   "source": [
    "## Prelim Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SKx-tObeKxH",
    "outputId": "67d02a73-9a19-40aa-8a94-451c46c1d7d0"
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForVision2Seq.from_pretrained(model_id,\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\").to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMFRsCOjgSJl"
   },
   "outputs": [],
   "source": [
    "# Regular expression pattern to extract assistant response\n",
    "answer_pattern = re.compile(r\"Assistant:\\s*(\\w+)\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "total = len(test_ds)\n",
    "\n",
    "# Function to process each entry\n",
    "def evaluate(entry):\n",
    "    global correct\n",
    "    global preds\n",
    "    img_path = os.path.join(\"visual-spatial-reasoning/images\", entry[\"image\"])\n",
    "\n",
    "    output = run_inference(img_path, entry[\"caption\"])\n",
    "\n",
    "    match = answer_pattern.search(output)\n",
    "    answer = match.group(1) if match else None\n",
    "\n",
    "    if answer == \"True\":\n",
    "      preds.append(1)\n",
    "    else:\n",
    "      preds.append(0)\n",
    "\n",
    "    # Validate answer\n",
    "    if answer in {\"True\", \"False\"} and (answer == \"True\") == (entry[\"label\"] == 1):\n",
    "        correct += 1\n",
    "\n",
    "# Process dataset with tqdm for progress tracking\n",
    "for entry in tqdm(test_ds, desc=\"Processing images\"):\n",
    "    evaluate(entry)\n",
    "\n",
    "# save preds\n",
    "with open(\"preds.txt\", \"w\") as f:\n",
    "    for i in range(len(preds)):\n",
    "        f.write(str(preds[i])+\"\\n\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Total images: {total}\")\n",
    "print(f\"Correct: {correct} ({correct / total:.2%} accuracy)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get an accuracy of 66%. Not terrible, but def below human performance of 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nssmMfibuRLo",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of this is taken straight from the following notebook: https://github.com/huggingface/smollm/blob/main/vision/finetuning/Smol_VLM_FT.ipynb\n",
    "\n",
    "In this case we are fine-tuning the instruct model rather than base since we want it to be able to answer spatial queries in a Q&A format which Instruct has already been made for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GctRy7LQuKFE"
   },
   "outputs": [],
   "source": [
    "# If you want to deploy the model, you should not quantize to 4bits since converting to onnx not supported\n",
    "\n",
    "USE_QLORA = False\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "    use_dora=False if USE_QLORA else True,\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "lora_config.inference_mode = False\n",
    "\n",
    "if USE_QLORA:\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        _attn_implementation=\"flash_attention_2\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "else:\n",
    "\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype = torch.bfloat16,\n",
    "        _attn_implementation=\"flash_attention_2\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "model.enable_adapters()\n",
    "\n",
    "if USE_QLORA:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(model.get_nb_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collate function is pretty important when you work with batch training. It defines the structure of your prompts and how to combine them for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t84Y4VvkxEr2"
   },
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \n",
    "  texts = []\n",
    "  images = []\n",
    "\n",
    "  for example in examples:\n",
    "\n",
    "      image_path = os.path.join(\"visual-spatial-reasoning/images\", example[\"image\"])\n",
    "\n",
    "      if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "      image = load_image(image_path)\n",
    "\n",
    "      if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "      caption = example[\"caption\"]\n",
    "      label = \"True.\" if example[\"label\"] == 1 else \"False.\"\n",
    "\n",
    "      messages = [\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"image\"},\n",
    "                  {\"type\": \"text\", \"text\": f\"{caption.rstrip('.')}, true or false?\"}\n",
    "              ]\n",
    "          },\n",
    "          {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": label}\n",
    "              ]\n",
    "          }\n",
    "      ]\n",
    "      text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "\n",
    "      texts.append(text.strip())\n",
    "      images.append([image])\n",
    "\n",
    "  batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "  labels = batch[\"input_ids\"].clone()\n",
    "  labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "  labels[labels == image_token_id] = -100\n",
    "  batch[\"labels\"] = labels\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGizX6Mgw66l"
   },
   "outputs": [],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\" if USE_QLORA else \"adamw_hf\", # for 8-bit, keep this, else adamw_hf\n",
    "    bf16=True, # underlying precision for 8bit\n",
    "    output_dir=f\"./{model_name}-vsr\",\n",
    "    hub_model_id=f\"{model_name}-vsr\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train! On a single A100, this took me about 3.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output model params should be saved in the directory SmolVLM-Instruct-vsr/checkpoint-480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Re-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"SmolVLM-Instruct-vsr/checkpoint-5\")\n",
    "model = model.merge_and_unload()\n",
    "model = model.to(torch.bfloat16).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "preds = []\n",
    "total = len(test_ds)\n",
    "\n",
    "# Process dataset with tqdm for progress tracking\n",
    "for entry in tqdm(test_ds, desc=\"Processing images\"):\n",
    "    evaluate(entry)\n",
    "\n",
    "# save preds\n",
    "with open(\"preds_post.txt\", \"w\") as f:\n",
    "    for i in range(len(preds)):\n",
    "        f.write(str(preds[i])+\"\\n\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Total images: {total}\")\n",
    "print(f\"Correct: {correct} ({correct / total:.2%} accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved the accuracy from 55% to 70% on the test set after fine-tuning! Nice.\n",
    "We trained for about 2hrs, so this is decent. We have to keep in mind that spatial awareness is not an easy task and in some ways can be subjective depending on perspective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "image1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "image2 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\n",
    "\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create input messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe the images?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\n",
    "\n",
    "for k,v in inputs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\"\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model, \"SmolVLM-Instruct-vsr/checkpoint-5\")\n",
    "# model = model.merge_and_unload()\n",
    "# model = model.to(torch.bfloat16)  # Keep dtype in bfloat16\n",
    "\n",
    "# Convert to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to CPU before export\n",
    "model.to(DEVICE)\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "tensor_inputs = {\n",
    "    \"pixel_values\": inputs[\"pixel_values\"].to(torch.float32),\n",
    "    \"pixel_attention_mask\": inputs[\"pixel_attention_mask\"].to(torch.float32),\n",
    "    \"input_ids\": inputs[\"input_ids\"],\n",
    "    \"attention_mask\": inputs[\"attention_mask\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Dynamic axes for variable batch size and sequence length\n",
    "dynamic_axes = {\n",
    "    \"pixel_values\": {0: \"batch_size\", 1: \"patches\"},\n",
    "    \"pixel_attention_mask\": {0: \"batch_size\", 1: \"patches\"},\n",
    "    \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "    \"output\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pass inputs as a tuple of tensors\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        tensor_inputs,  # Proper unpacking\n",
    "        \"smolvlm.onnx\",\n",
    "        input_names=[\"pixel_values\", \"pixel_attention_mask\", \"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        export_params=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "print(\"SmolVLM model exported to smolvlm.onnx\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "dQzTDAqdETgK",
    "0PfZvPMpESfB",
    "b1D1O6nKHOlO"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
